# **ğŸ‹ï¸ Sword ML Engineer Test**
> **Data Transformation and AI Message Generation Pipeline**

## **ğŸ“Œ Index**
1. [Project Overview](#project-overview)
2. [Repository Structure](#repository-structure)
3. [Branching Strategy](#branching-strategy)
4. [Setup Instructions](#setup-instructions)
5. [Data Transformation](#data-transformation)
6. [AI Message Generation](#ai-message-generation)
7. [Running the Pipeline](#running-the-pipeline)
8. [Testing](#testing)
9. [Future Improvements](#future-improvements)

---

## **ğŸ“Œ Project Overview**
This project automates **patient follow-up messaging** based on **session performance data** by leveraging:
- **Data Transformation:** Aggregating raw session data (`exercise_results.parquet`) into a structured format using **DuckDB & SQL**.
- **AI Message Generation:** Generating **personalized messages** for patients using **OpenAI GPT-4**.

The goal is to:
âœ”ï¸ **Enable data-driven decision-making** for PTs  
âœ”ï¸ **Ensure consistent & engaging** patient communication  
âœ”ï¸ **Reduce manual workload** by automating messaging  

---

## **ğŸ‘¤ Repository Structure**
```
ml-engineer-test/
â”‚â”€â”€ message/                        # Core application logic
â”‚   â”œâ”€â”€ data.py                     # Data transformation logic
â”‚   â”œâ”€â”€ main.py                     # Entry point for commands (transform, get_message)
â”‚   â”œâ”€â”€ model.py                    # OpenAI model integration
â”‚   â”œâ”€â”€ prompt_manager.py           # Manages AI prompt templates
|   â”œâ”€â”€ config.py    
â”‚â”€â”€ prompts/                        # AI message templates
â”‚   â”œâ”€â”€ system_prompt.txt
â”‚   â”œâ”€â”€ user_prompt.txt
â”‚   â”œâ”€â”€ scenario_ok.txt
â”‚   â”œâ”€â”€ scenario_nok.txt
â”‚â”€â”€ data/                           # Raw & transformed datasets
â”‚   â”œâ”€â”€ exercise_results.parquet    # Input
â”‚   â”œâ”€â”€ features_expected.parquet   # Expected Output
â”‚   â”œâ”€â”€ features.parquet            # Output
â”‚â”€â”€ notebooks/                      # Jupyter Notebooks for debugging
â”‚   â”œâ”€â”€ data-transformation.ipynb   # Testing for Question 1
â”‚   â”œâ”€â”€ message-generation.ipynb    # Testing for Question 2
â”‚â”€â”€ queries/                        # SQL queries for DuckDB
â”‚â”€â”€ tests/                          # Unit tests for queries
â”‚   â”œâ”€â”€ test_transformations.py 
â”‚â”€â”€ Makefile                        # Command-line automation
â”‚â”€â”€ requirements.txt                # Dependencies
â”‚â”€â”€ README.md                       # Project documentation
```

---

## **ğŸŒ± Branching Strategy**
This project follows a **feature-branch workflow**:

| Branch                          | Purpose                                   |
|---------------------------------|-------------------------------------------|
| **main**                        | Stable production-ready code              |
| **feature/data-transformation** | Question 1                                |
| **feature/ai-generated-message**| Question 2                                |
| **Documentation**               | Organizing Readme                         |

Merging was done via **separate PRs per feature branch** to keep changes modular and reviewable, and to simmulate real productization scenario.

---

## **âš™ï¸ Setup Instructions**
### **1ï¸âƒ£ Clone the repository**
```bash
git clone <your-repo-url>
cd ml-engineer-test
```

### **2ï¸âƒ£ Create & Activate Virtual Environment**
```bash
make venv  # Automatically creates and activates a virtual environment
source venv/bin/activate 
```

### **3ï¸âƒ£ Install Dependencies**
```bash
pip install -r requirements.txt
```

### **4ï¸âƒ£ Setup Environment Variables**
Create a `.env` file in the project root:
```bash
touch .env
```
Add your **OpenAI API Key** inside `.env`:
```
OPENAI_API_KEY=your-openai-key-here
```

---

## **ğŸ‹ï¸ Data Transformation Summary**

### **ğŸ’¡ Key Objectives**
âœ”ï¸ **Convert raw exercise records into structured session-level features**  
âœ”ï¸ **Compute essential performance metrics** (e.g., `perc_correct_repeats`, `training_time`, `quality_rating`)  
âœ”ï¸ **Identify skipped exercises & the most incorrect movements** 
âœ”ï¸ **Create unit tests for all transformations done**  
âœ”ï¸ **Validate alignment with `features_expected.parquet`**  

---

## **ğŸ›  Transformation Pipeline Overview**  

### **1. Data Aggregation & Grouping**  
- Collected **patient session data**.  
- Grouped by **`session_group`** to unify related exercises into a single record.  

### **2. Feature Engineering**  
- Created new **session-level metrics**:  
  - **`leave_exercise_*`** (number of left exercises).
  - **`prescribed_repeats*`** (number of repetitions supposed to perform). 
  - **`training_time`** (total session duration).  
  - **`perc_correct_repeats`** (accuracy percentage).  
  - **`number_exercises`** (total exercises performed).  
  - **`number_of_distinct_exercises`** (unique exercises).  

### **3. Handling Missing & Edge Cases**  
- Used **`COALESCE`** to fill missing values.  
- Ensured divisions avoid **`NULL` errors** using **`NULLIF`** in calculations.  

### **4. Ranking & Selection**  
- Identified **`exercise_with_most_incorrect`** using **ranking logic**.  
- **Randomly selected** an exercise in case of ties.  

### **5. Identifying Skipped Exercises**  
- Tracked **`first_exercise_skipped`** by ordering skipped exercises.  
- Selected the **first occurrence** using **`ROW_NUMBER()`**.  

### **6. Data Merging & Final Assembly**  
- **Merged all derived metrics** using **`LEFT JOIN`**.  
- Ensured **comprehensive session information** was retained.  

---

### **ğŸ“ SQL Transformation**
Stored in **`queries/features.sql`**, executed via **DuckDB**.


### **ğŸ›  Python Execution**
The transformation is executed in **`transform_features_sql()`**:

```python
@app.command()
def transform():
    exercise_df = pd.read_parquet(Path(DATA_DIR, "exercise_results.parquet"))
    transform_features_sql(tables_to_register=[("exercise_results", exercise_df)])
```

---

### **ğŸ” Validation & Testing**
To ensure correctness, we:
âœ” **Compare `features.parquet` vs `features_expected.parquet` using Pandas**  
âœ” **Validate key aggregations using unit tests (`pytest`)**  
âœ” **Unit tests where done for all SQL Queries (`test_transformations.py`)**  


### **ğŸ“Š Jupyter Notebook Comparisons**
We **compared outputs** for correctness, being all columns matching apart from `exercise_with_most_incorrect` which is expectedly different since "If there are two with the highest number of incorrect movement, you can pick any of them.", so with the randomness they will not be the same.
```python

# Load the parquet files
df_expected = pd.read_parquet("../data/features_expected.parquet")
df_actual = pd.read_parquet("../data/features.parquet")

# Find differing columns
differences = df_expected.compare(df_actual, keep_shape=True, keep_equal=False)
print("Columns with differences:", differences)
```

---
